{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 - NB-SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = '/data/datasets/kaggle/jigsaw-toxic-comment-classification-challenge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(BASEDIR, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(BASEDIR, 'test.csv'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.585100e+04</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.994359e+11</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.053301</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.897862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.890136e+11</td>\n",
       "      <td>0.295097</td>\n",
       "      <td>0.099832</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.217352</td>\n",
       "      <td>0.091762</td>\n",
       "      <td>0.302831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.225664e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.473437e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.001297e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.501088e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999882e+11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         toxic  severe_toxic       obscene        threat  \\\n",
       "count  9.585100e+04  95851.000000  95851.000000  95851.000000  95851.000000   \n",
       "mean   4.994359e+11      0.096368      0.010068      0.053301      0.003182   \n",
       "std    2.890136e+11      0.295097      0.099832      0.224635      0.056320   \n",
       "min    2.225664e+07      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    2.473437e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    5.001297e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    7.501088e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "max    9.999882e+11      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             insult  identity_hate          none  \n",
       "count  95851.000000   95851.000000  95851.000000  \n",
       "mean       0.049713       0.008492      0.897862  \n",
       "std        0.217352       0.091762      0.302831  \n",
       "min        0.000000       0.000000      0.000000  \n",
       "25%        0.000000       0.000000      1.000000  \n",
       "50%        0.000000       0.000000      1.000000  \n",
       "75%        0.000000       0.000000      1.000000  \n",
       "max        1.000000       1.000000      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95851, 226998)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_double_max(text):\n",
    "    \"\"\"Removes unecessary doubling/tripling/etc of characters\n",
    "    \n",
    "    Steps:\n",
    "        1. Replaces every 3+ consecutive identical chars by 2 consecutive identical chars\n",
    "        2. Replaces every 2+ consecutive non-word character by a single\n",
    "    \"\"\"\n",
    "    import re\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "    return re.sub(r'(\\W)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    \"\"\"Applies all preprocessing rules to the corpus\"\"\"\n",
    "    corpus = (reduce_to_double_max(s.lower()) for s in corpus)\n",
    "    docs = nlp.pipe(corpus, batch_size=1000, n_threads=12)\n",
    "    return [' '.join([x.lemma_ for x in doc if x.is_alpha]) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train_processed = '../data/processed/train.txt'\n",
    "\n",
    "if os.path.isfile(fname_train_processed):\n",
    "    with open(fname_train_processed, 'r') as fin:\n",
    "        train_processed = [line.strip() for line in fin if line]\n",
    "    \n",
    "else:\n",
    "    train_processed = preprocess_corpus(train['comment_text'])\n",
    "\n",
    "    with open(fname_train_processed, 'w') as fout:\n",
    "        for doc in train_processed:\n",
    "            fout.write('{}\\n'.format(doc))\n",
    "    \n",
    "train['comment_text_processed'] = train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_test_processed = '../data/processed/test.txt'\n",
    "\n",
    "if os.path.isfile(fname_test_processed):\n",
    "    with open(fname_test_processed, 'r') as fin:\n",
    "        test_processed = [line.strip() for line in fin if line]\n",
    "    \n",
    "else:\n",
    "    test_processed = preprocess_corpus(test['comment_text'])\n",
    "\n",
    "    with open(fname_test_processed, 'w') as fout:\n",
    "        for doc in test_processed:\n",
    "            fout.write('{}\\n'.format(doc))\n",
    "    \n",
    "test['comment_text_processed'] = test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([train['comment_text_processed'], test['comment_text_processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.75, max_features=100000, min_df=10,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vect = TfidfVectorizer(\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    min_df=10,\n",
    "    max_df=0.75,\n",
    "    ngram_range=(1,2),\n",
    "    max_features=100000,\n",
    "    binary=True)\n",
    "word_vect.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features = word_vect.transform(train['comment_text_processed'])\n",
    "test_word_features = word_vect.transform(test['comment_text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100000, min_df=1,\n",
       "        ngram_range=(1, 5), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vect = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1,5),\n",
    "    max_features=100000)\n",
    "char_vect.fit(pd.concat([train['comment_text'], test['comment_text']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_features = char_vect.transform(train['comment_text'])\n",
    "test_char_features = char_vect.transform(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack((train_char_features, train_word_features))\n",
    "test_features = hstack((test_char_features, test_word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from AlexSanchez's code at https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline-eda-0-052-lb\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x.tocsr()[y==y_i, :].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, solver='sag', dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_ft, y_true):\n",
    "    cv_loss = np.mean(cross_val_score(model, train_ft, y_true, cv=3, n_jobs=4, scoring='neg_log_loss'))\n",
    "    return cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. CV loss for class toxic: -0.10059793434871923\n",
      "(95851, 200000) (95851,)\n",
      "(95851, 200000) (95851,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63902, 200000) (63902,)\n",
      "(63902, 200000) (63902,)\n",
      "Avg. CV loss for class severe_toxic: -0.03118636050896448\n",
      "(95851, 200000) (95851,)\n",
      "(95851, 200000) (95851,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "Avg. CV loss for class obscene: -0.05851601719440596\n",
      "(95851, 200000) (95851,)\n",
      "(95851, 200000) (95851,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63902, 200000) (63902,)\n",
      "(63902, 200000) (63902,)\n",
      "Avg. CV loss for class threat: -0.012098497638420616\n",
      "(95851, 200000) (95851,)\n",
      "(95851, 200000) (95851,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "Avg. CV loss for class insult: -0.07934594923123112\n",
      "(95851, 200000) (95851,)\n",
      "(95851, 200000) (95851,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63900, 200000) (63900,)\n",
      "(63900, 200000) (63900,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "(63901, 200000) (63901,)\n",
      "Avg. CV loss for class identity_hate: -0.028611218202053923\n",
      "(95851, 200000) (95851,)\n",
      "(95851, 200000) (95851,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "preds = {'id': test['id']}\n",
    "for class_name in class_names:\n",
    "    targets = train[class_name]\n",
    "    model = NbSvmClassifier(C=4, dual=False)\n",
    "    loss = evaluate_model(model, train_features, targets)\n",
    "    print('Avg. CV loss for class {}: {}'.format(class_name, loss))\n",
    "    losses.append(loss)\n",
    "    model.fit(train_features, targets)\n",
    "    preds[class_name] = model.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Avg. CV loss: -0.05172599618729923\n"
     ]
    }
   ],
   "source": [
    "print('Cumulative Avg. CV loss: {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "submission = pd.DataFrame.from_dict(preds)\n",
    "submission.to_csv('../data/external/submission-{}.csv'.format(time.strftime('%Y%m%d_%H%M', time.localtime())), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
