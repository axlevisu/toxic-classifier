{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Basic Exploration & Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = '/data/datasets/kaggle/jigsaw-toxic-comment-classification-challenge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first inspect the training set and gather basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(BASEDIR, 'train.csv'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395.34186393464859, 595.10207169971216, 5000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = train.comment_text.str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b0835ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(os.path.join(BASEDIR, 'test.csv'))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].fillna(' ')\n",
    "test['comment_text'] = test['comment_text'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0   6044863    0.5           0.5      0.5     0.5     0.5            0.5\n",
       "1   6102620    0.5           0.5      0.5     0.5     0.5            0.5\n",
       "2  14563293    0.5           0.5      0.5     0.5     0.5            0.5\n",
       "3  21086297    0.5           0.5      0.5     0.5     0.5            0.5\n",
       "4  22982444    0.5           0.5      0.5     0.5     0.5            0.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(os.path.join(BASEDIR, 'sample_submission.csv'))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis\n",
    "This is a multilabel classification task, so let's check the proportion of each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.0963683216659\n",
      "severe_toxic 0.0100677092571\n",
      "obscene 0.0533014783362\n",
      "threat 0.0031820220968\n",
      "insult 0.0497125747254\n",
      "identity_hate 0.00849234749768\n"
     ]
    }
   ],
   "source": [
    "for label in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    print(label, (train[label] == 1.0).sum() / len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308810</td>\n",
       "      <td>0.677491</td>\n",
       "      <td>0.162967</td>\n",
       "      <td>0.648330</td>\n",
       "      <td>0.259124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404540</td>\n",
       "      <td>0.133469</td>\n",
       "      <td>0.377450</td>\n",
       "      <td>0.193385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.677491</td>\n",
       "      <td>0.404540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149874</td>\n",
       "      <td>0.744685</td>\n",
       "      <td>0.287794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.162967</td>\n",
       "      <td>0.133469</td>\n",
       "      <td>0.149874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>0.123971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.648330</td>\n",
       "      <td>0.377450</td>\n",
       "      <td>0.744685</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.331922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.259124</td>\n",
       "      <td>0.193385</td>\n",
       "      <td>0.287794</td>\n",
       "      <td>0.123971</td>\n",
       "      <td>0.331922</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308810  0.677491  0.162967  0.648330   \n",
       "severe_toxic   0.308810      1.000000  0.404540  0.133469  0.377450   \n",
       "obscene        0.677491      0.404540  1.000000  0.149874  0.744685   \n",
       "threat         0.162967      0.133469  0.149874  1.000000  0.157534   \n",
       "insult         0.648330      0.377450  0.744685  0.157534  1.000000   \n",
       "identity_hate  0.259124      0.193385  0.287794  0.123971  0.331922   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.259124  \n",
       "severe_toxic        0.193385  \n",
       "obscene             0.287794  \n",
       "threat              0.123971  \n",
       "insult              0.331922  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=30000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents='unicode', token_pattern='\\\\w{2,}', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    ngram_range=(1,1),\n",
    "    token_pattern=r'\\w{2,}',\n",
    "    max_features=30000)\n",
    "token_counts.fit(train['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = token_counts.fit_transform(train['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[484, 1160, 92, 12, 13]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freq = X.sum(axis=0).tolist()[0]\n",
    "token_freq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts_list = [(k, token_freq[v]) for k, v in token_counts.vocabulary_.items()]\n",
    "token_counts_list = sorted(token_counts_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 298519),\n",
       " ('to', 178473),\n",
       " ('of', 135159),\n",
       " ('and', 134598),\n",
       " ('you', 132255),\n",
       " ('is', 106191),\n",
       " ('that', 97203),\n",
       " ('it', 89201),\n",
       " ('in', 87442),\n",
       " ('for', 61943),\n",
       " ('this', 58655),\n",
       " ('not', 56674),\n",
       " ('on', 54089),\n",
       " ('be', 50228),\n",
       " ('as', 46431),\n",
       " ('are', 44062),\n",
       " ('have', 43460),\n",
       " ('your', 37761),\n",
       " ('with', 36144),\n",
       " ('if', 35336),\n",
       " ('article', 34410),\n",
       " ('was', 32727),\n",
       " ('or', 32474),\n",
       " ('but', 30546),\n",
       " ('wikipedia', 29250)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts_list[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vbe', 5),\n",
       " ('ohinternet', 5),\n",
       " ('opec', 5),\n",
       " ('zj', 5),\n",
       " ('orkhon', 5),\n",
       " ('harawira', 5),\n",
       " ('nomme', 5),\n",
       " ('mikenorton', 5),\n",
       " ('catt', 5),\n",
       " ('primrose', 5),\n",
       " ('hue', 5),\n",
       " ('babli', 5),\n",
       " ('sparxent', 5),\n",
       " ('prsa', 5),\n",
       " ('pietruczuk', 5),\n",
       " ('rasche', 5),\n",
       " ('heiemo', 5),\n",
       " ('lra', 5),\n",
       " ('corpsefucking', 5),\n",
       " ('tielu', 5),\n",
       " ('matthewfenton', 5),\n",
       " ('miquonranger03', 5),\n",
       " ('vcite', 5),\n",
       " ('cozette', 5),\n",
       " ('dwpaul', 5)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts_list[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_double_max(text):\n",
    "    \"\"\"Removes unecessary doubling/tripling/etc of characters\n",
    "    \n",
    "    Steps:\n",
    "        1. Replaces every 3+ consecutive identical chars by 2 consecutive identical chars\n",
    "        2. Replaces every 2+ consecutive non-word character by a single\n",
    "    \"\"\"\n",
    "    import re\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "    return re.sub(r'(\\W)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    \"\"\"Applies all preprocessing rules to the corpus\"\"\"\n",
    "    corpus = (reduce_to_double_max(s.lower()) for s in corpus)\n",
    "    docs = nlp.pipe(corpus, batch_size=1000, n_threads=12)\n",
    "    return [' '.join([x.lemma_ for x in doc if x.is_alpha]) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sample_train['comment_text_processed'] = preprocess_corpus(sample_train['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsense kiss off geek what i say be true -PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please do not vandalize page as -PRON- do with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>point of interest i remove the point of intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ask some -PRON- nationality be a racial offenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the reader here be not go by -PRON- say so for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                              comment_text_processed  \n",
       "0  nonsense kiss off geek what i say be true -PRO...  \n",
       "1  please do not vandalize page as -PRON- do with...  \n",
       "2  point of interest i remove the point of intere...  \n",
       "3  ask some -PRON- nationality be a racial offenc...  \n",
       "4  the reader here be not go by -PRON- say so for...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train_processed = '../data/processed/train.txt'\n",
    "\n",
    "if os.path.isfile(fname_train_processed):\n",
    "    with open(fname_train_processed, 'r') as fin:\n",
    "        train_processed = [line.strip() for line in fin if line]\n",
    "    \n",
    "else:\n",
    "    train_processed = preprocess_corpus(train['comment_text'])\n",
    "\n",
    "    with open(fname_train_processed, 'w') as fout:\n",
    "        for doc in train_processed:\n",
    "            fout.write('{}\\n'.format(doc))\n",
    "    \n",
    "train['comment_text_processed'] = train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_test_processed = '../data/processed/test.txt'\n",
    "\n",
    "if os.path.isfile(fname_test_processed):\n",
    "    with open(fname_test_processed, 'r') as fin:\n",
    "        test_processed = [line.strip() for line in fin if line]\n",
    "    \n",
    "else:\n",
    "    test_processed = preprocess_corpus(test['comment_text'])\n",
    "\n",
    "    with open(fname_test_processed, 'w') as fout:\n",
    "        for doc in test_processed:\n",
    "            fout.write('{}\\n'.format(doc))\n",
    "    \n",
    "test['comment_text_processed'] = test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([train['comment_text_processed'], test['comment_text_processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100000, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{2,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vect = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',\n",
    "    ngram_range=(1,2),\n",
    "    max_features=100000,\n",
    "    binary=True)\n",
    "word_vect.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features = word_vect.transform(train['comment_text_processed'])\n",
    "test_word_features = word_vect.transform(test['comment_text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 4), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vect = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1,4),\n",
    "    max_features=50000)\n",
    "char_vect.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_features = char_vect.transform(train['comment_text_processed'])\n",
    "test_char_features = char_vect.transform(test['comment_text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack((train_char_features, train_word_features))\n",
    "test_features = hstack((test_char_features, test_word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, y_true, train_ft):\n",
    "    cv_loss = np.mean(cross_val_score(model, train_ft, y_true, cv=3, n_jobs=4, scoring='neg_log_loss'))\n",
    "    return cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. CV loss for class toxic: -0.10742453692283375\n",
      "Avg. CV loss for class severe_toxic: -0.02735634377924882\n",
      "Avg. CV loss for class obscene: -0.05859598700532948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. CV loss for class threat: -0.010322841418776043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceman/.pyenv/versions/3.6.2/envs/toxic/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. CV loss for class insult: -0.07822685518477351\n",
      "Avg. CV loss for class identity_hate: -0.024816074351928202\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "preds = {'id': test['id']}\n",
    "for class_name in class_names:\n",
    "    targets = train[class_name]\n",
    "    model = LogisticRegression(C=4.5, solver='sag')\n",
    "    loss = evaluate_model(model, targets, train_features)\n",
    "    print('Avg. CV loss for class {}: {}'.format(class_name, loss))\n",
    "    losses.append(loss)\n",
    "    model.fit(train_features, targets)\n",
    "    preds[class_name] = model.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Avg. CV loss: -0.05112377311048164\n"
     ]
    }
   ],
   "source": [
    "print('Cumulative Avg. CV loss: {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "submission.to_csv('../data/external/submission-{}.csv'.format(time.strftime('%Y%m%d_%H%M', time.localtime())), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
